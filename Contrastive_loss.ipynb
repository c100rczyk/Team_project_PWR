{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T17:37:12.313247Z",
     "start_time": "2024-03-23T17:37:12.309743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "print(\"hello\")\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4eabe-6121-448b-9b0c-6f842ea3c771",
   "metadata": {},
   "source": [
    "Structure of files:\n",
    "\n",
    "|->Contrastive_loss.ipynb \n",
    "|-> Fruits_test\n",
    "|      |\n",
    "|      |-> Banana\n",
    "|      |-> MuskMelon \n",
    "       ...\n",
    "There are no subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13f0800c-bae7-4ca3-8f80-e9be102d8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "margin = 1 # margin for contrastive loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2cf5a-460e-4869-a6a4-a8bfe2866fe7",
   "metadata": {},
   "source": [
    "# Create paths to appropriate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad71d2c8f393b273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T17:37:12.738622Z",
     "start_time": "2024-03-23T17:37:12.731310Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ContrastiveGenerator:\n",
    "    def __init__(self, datasetPath, number_of_pairs):       #number_of_pairs : number of iterations in loop next_get_item()\n",
    "        # empty list that will contain the subdirectory names of products\n",
    "        # of the dataset directory with more than one image in it\n",
    "        self.types_of_products = list()\n",
    "        self.number_of_pairs = number_of_pairs\n",
    "\n",
    "        #iterate over subdirectories in the main directory:\n",
    "        for folderName in os.listdir(datasetPath):\n",
    "            # if os.path.isdir(os.path.join(datasetPath, folderName)):\n",
    "            #     for filename in os.listdir(os.path.join(datasetPath, folderName)):\n",
    "            #         absoluteFolderName = datasetPath + \"/\" + folderName + \"/\" + filename\n",
    "            #         numImages = len(os.listdir(absoluteFolderName))\n",
    "            #         if numImages > 1:\n",
    "            #             self.types_of_products.append(absoluteFolderName)\n",
    "            # else:\n",
    "            absoluteFolderName = os.path.join(datasetPath, folderName) #datasetPath + \"/\" + folderName\n",
    "            # get the number of images in the subdirectory\n",
    "            numImages = len(os.listdir(absoluteFolderName))\n",
    "            if numImages > 1:\n",
    "                self.types_of_products.append(absoluteFolderName)\n",
    "\n",
    "        # create a dictionary of people name to their image names\n",
    "        self.allProducts = self.generate_all_products_dict()\n",
    "\n",
    "\n",
    "    # Budowanie ścieżek do konkretnych zdjęć\n",
    "    def generate_all_products_dict(self):\n",
    "        # create an empty dictionary that will be populated with\n",
    "        # directory names as keys and image names as values\n",
    "        all_products = dict()\n",
    "        for product in self.types_of_products:\n",
    "            image_names = os.listdir(product)\n",
    "            # build the image paths and populate the dictionary\n",
    "            productsPhotos = [os.path.join(product, imageName) for imageName in image_names]\n",
    "            all_products[product] = productsPhotos\n",
    "        print(len(all_products))\n",
    "        #print(all_products)\n",
    "        return all_products\n",
    "\n",
    "    def get_next_element(self):\n",
    "        for i in range(self.number_of_pairs):\n",
    "            anchor = random.choice(self.types_of_products)\n",
    "\n",
    "            # copy the list of products\n",
    "            temporaryImages = self.types_of_products.copy()\n",
    "            temporaryImages.remove(anchor)\n",
    "\n",
    "            # random product from a list of products without anchor\n",
    "            negativeProduct = random.choice(temporaryImages)\n",
    "\n",
    "            (anchorProduct, positiveProduct) = np.random.choice(\n",
    "                a=self.allProducts[anchor],\n",
    "                size=2,\n",
    "                replace=False\n",
    "            )\n",
    "            # Image from the negative folder\n",
    "            negativeProduct = random.choice(self.allProducts[negativeProduct])\n",
    "\n",
    "            # anchor = tf.convert_to_tensor(anchor)\n",
    "            # positiveProduct = tf.convert_to_tensor(positiveProduct)\n",
    "            # negativeProduct = tf.convert_to_tensor(negativeProduct)\n",
    "\n",
    "            yield (anchorProduct, positiveProduct, 0.0)\n",
    "            yield (anchorProduct, negativeProduct, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e1c33-00e4-4237-85cd-e24f9ac18b9d",
   "metadata": {},
   "source": [
    "# Create object for creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebf19d6c326282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T17:37:13.742069Z",
     "start_time": "2024-03-23T17:37:13.735542Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Fruits_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFruits_test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m number_of_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 3\u001b[0m generator \u001b[38;5;241m=\u001b[39m ContrastiveGenerator(path, number_of_samples)\n",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m, in \u001b[0;36mContrastiveGenerator.__init__\u001b[0;34m(self, datasetPath, number_of_pairs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_pairs \u001b[38;5;241m=\u001b[39m number_of_pairs\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#iterate over subdirectories in the main directory:\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folderName \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(datasetPath):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# if os.path.isdir(os.path.join(datasetPath, folderName)):\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#     for filename in os.listdir(os.path.join(datasetPath, folderName)):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#         absoluteFolderName = datasetPath + \"/\" + folderName + \"/\" + filename\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#         numImages = len(os.listdir(absoluteFolderName))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#         if numImages > 1:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#             self.types_of_products.append(absoluteFolderName)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     absoluteFolderName \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(datasetPath, folderName) \u001b[38;5;66;03m#datasetPath + \"/\" + folderName\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# get the number of images in the subdirectory\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Fruits_test'"
     ]
    }
   ],
   "source": [
    "path = r\"Fruits_test\"\n",
    "number_of_samples = 2\n",
    "generator = ContrastiveGenerator(path, number_of_samples)\n",
    "# for positive_pair, negative_pair in generator.get_next_element():\n",
    "#     print(positive_pair)\n",
    "#     print(negative_pair)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134a469-c1a2-4541-8ac2-575fdd279db2",
   "metadata": {},
   "source": [
    "# Generator for creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d36c8d87fae2d64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_generator( generator\u001b[38;5;241m.\u001b[39mget_next_element,\n\u001b[1;32m      2\u001b[0m                                           output_signature\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m(), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mstring),\n\u001b[1;32m      3\u001b[0m                                                             tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m(), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mstring),\n\u001b[1;32m      4\u001b[0m                                                             tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m(), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_generator( generator.get_next_element,\n",
    "                                          output_signature=(tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "                                                            tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "                                                            tf.TensorSpec(shape=(), dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755d40b-db2f-4525-a292-1f41346a40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067af6d-38b8-4958-970b-a7e4671e352e",
   "metadata": {},
   "source": [
    "# Creating dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1a6c40-e312-4f3d-8d4a-f1db83a2277e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_products' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m         list_of_products\u001b[38;5;241m.\u001b[39mappend( ((img1, img2),value\u001b[38;5;241m.\u001b[39mnumpy() ) )\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m list_of_products\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(list_of_products[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(list_of_products))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_products' is not defined"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (258, 320)\n",
    "\n",
    "\n",
    "\n",
    "def decode_and_resize(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    #img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def decode_resize_wrapper(image_path1, image_path2,value):\n",
    "    # return tf.py_function(decode_and_resize, [image_path], tf.float32)\n",
    "    img1 = decode_and_resize(image_path1)\n",
    "    img2 = decode_and_resize(image_path2)\n",
    "    return (img1, img2), value\n",
    "\n",
    "def generate_new_pack_of_images():\n",
    "    list_of_products = []\n",
    "    \n",
    "    # Mapowanie funkcji na oba obrazy\n",
    "    img_pack = dataset.map(lambda image_path1, image_path2, value: (decode_resize_wrapper(image_path1,image_path2,value)) )\n",
    "\n",
    "    # training = img_pack.map(lambda x, y: (x,y))\n",
    "    # print(training)\n",
    "    # return training\n",
    "    for (img1, img2), value in img_pack:\n",
    "        list_of_products.append( ((img1, img2),value.numpy() ) )\n",
    "        \n",
    "    return list_of_products\n",
    "\n",
    "# print(list_of_products[0])\n",
    "# print(len(list_of_products))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b891194-3c72-407b-b488-d8658d75be8e",
   "metadata": {},
   "source": [
    "# Create new pack of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ed5a2c-9744-4625-b915-8e376e47f086",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m list_of_products \u001b[38;5;241m=\u001b[39m generate_new_pack_of_images()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(list_of_products[0])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([item[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m list_of_products])\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mgenerate_new_pack_of_images\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m list_of_products \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Mapowanie funkcji na oba obrazy\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m img_pack \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m image_path1, image_path2, value: (decode_resize_wrapper(image_path1,image_path2,value)) )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# training = img_pack.map(lambda x, y: (x,y))\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(training)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# return training\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (img1, img2), value \u001b[38;5;129;01min\u001b[39;00m img_pack:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "list_of_products = generate_new_pack_of_images()\n",
    "# print(list_of_products[0])\n",
    "\n",
    "X = np.array([item[:-1] for item in list_of_products])\n",
    "y = np.array([item[-1] for item in list_of_products])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e9bf6-ac8d-44c0-b35b-8c008a336c50",
   "metadata": {},
   "source": [
    "## show and check if everything is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d966a3-87ad-4758-a081-bd10449b0489",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_products' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# wyświetlenie obrazów:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m (img1, img2), label \u001b[38;5;241m=\u001b[39m list_of_products[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Obraz 1\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_products' is not defined"
     ]
    }
   ],
   "source": [
    "# wyświetlenie obrazów:\n",
    "import numpy\n",
    "(img1, img2), label = list_of_products[0]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Obraz 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img1)\n",
    "plt.title('Image 1')\n",
    "# Obraz 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img2)\n",
    "plt.title('Image 2')\n",
    "plt.show()\n",
    "print(\"Etykieta:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74321b95-b0cb-485f-b0d9-6babc6f67c5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_products' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (img1, img2), label \u001b[38;5;241m=\u001b[39m list_of_products[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Wyświetlenie obrazów\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_products' is not defined"
     ]
    }
   ],
   "source": [
    "(img1, img2), label = list_of_products[1]\n",
    "\n",
    "# Wyświetlenie obrazów\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Obraz 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img1)\n",
    "plt.title('Image 1')\n",
    "# Obraz 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img2)\n",
    "plt.title('Image 2')\n",
    "\n",
    "plt.show()\n",
    "print(\"Etykieta:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4160989-99c0-4c50-8115-33e65650a923",
   "metadata": {},
   "source": [
    "# model for products distinguish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "59ee102b-f4ca-4d61-988c-70f8b01e2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"\n",
    "    Znalezienie odległości euklidesowej pomiędzy dwoma wektorami:\n",
    "\n",
    "    Arguments:\n",
    "        vects: Lista zawierające dwa tensory tej samej długości\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance pomiędzy podanymi wektorami\n",
    "    \"\"\"\n",
    "    x, y = vects\n",
    "\n",
    "    sum_square = ops.sum(ops.square(x-y), axis=1, keepdims=True)\n",
    "    return ops.sqrt(ops.maximum(sum_square, keras.backend.epsilon()))\n",
    "\n",
    "\n",
    "input = keras.layers.Input((258,320,3))\n",
    "x = keras.layers.BatchNormalization()(input)\n",
    "x = keras.layers.Conv2D(16, (5,5), activation=\"tanh\")(x)  # ilość elementów filtra, rozmiar filtra\n",
    "x = keras.layers.AveragePooling2D(pool_size=(2,2))(x)\n",
    "x = keras.layers.Conv2D(32, (5,5), activation=\"tanh\")(x)\n",
    "x = keras.layers.AveragePooling2D(pool_size=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dense(10, activation=\"tanh\")(x)\n",
    "embedding_network = keras.Model(input, x)   # tworzę instancję modelu. Model nazywam embedding network\n",
    "                                            # używam tego do dalszego definiowania modelu\n",
    "\n",
    "input_1 = keras.layers.Input((258,320,3))\n",
    "input_2 = keras.layers.Input((258,320,3))\n",
    "\n",
    "tower_1 = embedding_network(input_1)\n",
    "tower_2 = embedding_network(input_2)\n",
    "\n",
    "merge_layer = keras.layers.Lambda(euclidean_distance, output_shape=(1,))([tower_1, tower_2])\n",
    "\n",
    "normal_layer = keras.layers.BatchNormalization()(merge_layer)\n",
    "output_layer = keras.layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "\n",
    "siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38e4e6-f897-4069-bd5c-18dbe826638e",
   "metadata": {},
   "source": [
    "# Definition of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e0ffe7af-3376-43d9-94e8-14fe00a85afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(margin=1):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "        shoud be classified as dissimilar \"1\"\n",
    "    Returns:\n",
    "        tensor with contrastive loss as floating point value\n",
    "    \"\"\"\n",
    "    # 0-same ,  1-different\n",
    "    \n",
    "    #contrastive_loss = (1-y_true)*\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculate the contrastive loss\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type \"float32\"\n",
    "            y_pred: List of predictions\n",
    "            y_pred to przewidywane odległości między parami danych, które model stara się nauczyć\n",
    "        Returns:\n",
    "            A tensor containing contrastive loss value (folat)\n",
    "        \"\"\"\n",
    "        square_pred = ops.square(y_pred)\n",
    "        margin_square = ops.square(ops.maximum(margin - (y_pred), 0))\n",
    "        return ops.mean((1 - y_true) * square_pred + (y_true) * margin_square)\n",
    "    return contrastive_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28add7-4aaa-4476-9467-6f33210ca609",
   "metadata": {},
   "source": [
    "## Compile model with contrastive loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "99467c1b-1b9e-498e-8c99-55e11d6e699e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_43\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_43\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_31      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_32      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_41       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,118,326</span> │ input_layer_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_layer_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ functional_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │ lambda_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ batch_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_31      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m258\u001b[0m, \u001b[38;5;34m320\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_32      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m258\u001b[0m, \u001b[38;5;34m320\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_41       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │  \u001b[38;5;34m2,118,326\u001b[0m │ input_layer_31[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_layer_32[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_10 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ functional_41[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ functional_41[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m4\u001b[0m │ lambda_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ batch_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,118,332</span> (8.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,118,332\u001b[0m (8.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,817,716</span> (6.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,817,716\u001b[0m (6.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">300,616</span> (1.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m300,616\u001b[0m (1.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile model with contrastive loss\n",
    "\n",
    "siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "04d09bbf-f4ea-46b5-a260-7b5986318f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer 'functional_43' expected 2 input(s). Received 1 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_products_list = list_of_products[:(int)(0.8*2*number_of_samples)]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# val_products_list = list_of_products[(int)(0.8*2*number_of_samples) :]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(x_train_1)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(labels_train)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m siamese\u001b[38;5;241m.\u001b[39mfit( X, y, \n\u001b[1;32m     28\u001b[0m             validation_split\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, \n\u001b[1;32m     29\u001b[0m             batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[1;32m     30\u001b[0m             epochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m     31\u001b[0m            )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/input_spec.py:156\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_spec) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Having a shape/dtype is the only commonality of the various\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# tensor-like objects that may be passed. The most common kind of\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# invalid type we are guarding for is a Layer instance (Functional API),\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# which does not have a `shape` attribute.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Layer 'functional_43' expected 2 input(s). Received 1 instead."
     ]
    }
   ],
   "source": [
    "# train_products_list = list_of_products[:(int)(0.8*2*number_of_samples)]\n",
    "# val_products_list = list_of_products[(int)(0.8*2*number_of_samples) :]\n",
    "\n",
    "# print(len(train_products_list))\n",
    "# print(len(val_products_list))\n",
    "\n",
    "# x_train_1 = []\n",
    "# x_train_2 = []\n",
    "# labels_train = []\n",
    "# x_val_1 = []\n",
    "# x_val_2 = []\n",
    "# labels_val = []\n",
    "# for (img1, img2), label in train_products_list:\n",
    "#     x_train_1.append(img1)\n",
    "#     x_train_2.append(img2)\n",
    "#     labels_train.append(label)\n",
    "# for (img1, img2), label in val_products_list:\n",
    "#     x_val_1.append(img1)\n",
    "#     x_val_2.append(img2)\n",
    "#     labels_val.append(label)\n",
    "\n",
    "\n",
    "# print(x_train_1)\n",
    "# print(labels_train)\n",
    "\n",
    "\n",
    "siamese.fit( X, y, \n",
    "            validation_split= 0.2, \n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54a9da-95da-4cc5-acb2-5af0cee8cb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598dfd5-92c4-4ede-b2ae-96fd097356d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
